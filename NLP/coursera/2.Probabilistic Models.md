


[参考视频](https://www.coursera.org/learn/probabilistic-models-in-nlp/)


- [Week1 Autocorrect](#week1-autocorrect)
  - [How to work](#how-to-work)
    - [1. Identify a misspelled word](#1-identify-a-misspelled-word)
    - [2. Find strings n edit distance away](#2-find-strings-n-edit-distance-away)
    - [3. Filter candidates](#3-filter-candidates)
  - [最小编辑距离](#最小编辑距离)
  - [PA(Autocorrect)](#paautocorrect)
  - [最短编辑距离的计算](#最短编辑距离的计算)
- [Week2 Markov Chains and POS Tags](#week2-markov-chains-and-pos-tags)
  - [Part of speech (POS) tagging](#part-of-speech-pos-tagging)
  - [Markov chains](#markov-chains)
  - [隐马尔可夫模型(HMM)](#隐马尔可夫模型hmm)
  - [计算转移矩阵A(transition matrix)](#计算转移矩阵atransition-matrix)
  - [计算发射矩阵B(emission matrix)](#计算发射矩阵bemission-matrix)
  - [Viterbi 算法](#viterbi-算法)


## Week1 Autocorrect

文本的自动纠错

### How to work

#### 1. Identify a misspelled word
本节只考虑简单的拼写错误：在给定的字典中找不到的单词。其他的错词，比如根据上下文推断出来的错误，要先不考虑。

#### 2. Find strings n edit distance away
长度为n的编辑距离，编辑方法包括: insert,delete,switch(swap 2 adjacent letters),replace<br>
这步要找到所有可能的n-edit distance的单词，通常n在1-3之间

#### 3. Filter candidates
过滤2中找到的所有词，删掉不在字典中的词

4. Calculate word probabilities
计算每一个词在整个语料库中出现的概率 （感觉这个方法比较粗糙）

### 最小编辑距离

编辑距离（Edit Distance），又称Levenshtein距离，是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。一般来说，编辑距离越小，两个串的相似度越大。对于不同的操作类型，可以使用不同的操作次数，比如插入一次算一个操作次数，替换一次算2个操作次数，这是可以自定义的。

计算最小编辑距离的方法就是DP

### PA(Autocorrect)
- 预处理
  - 从语料库获取数据，获取每个单词将其转为小写
  - 计算每个单词的计数
  - 计数每个单词的出现频率
     $P(w_i) = \frac{C(w_i)}{M}$ 
where $C(w_i)$ is the total number of times $w_i$ appears in the corpus.
$M$ is the total number of words in the corpus. M中包含重复单词的重复计数
  - 单词处理（插入/交换相邻/删除/替换），分为一次处理和二次处理
  - 拼写改正的推荐单词的优先级
    * If the word is in the vocabulary, suggest the word. 
    * Otherwise, if there are suggestions from `edit_one_letter` that are in the vocabulary, use those. 
    * Otherwise, if there are suggestions from `edit_two_letters` that are in the vocabulary, use those. 
    * Otherwise, suggest the input word.*  
    * The idea is that words generated from fewer edits are more likely than words with more edits.

### 最短编辑距离的计算
使用DP来计算，用来衡量纠正后的单词和纠正之前的差距
``` python
def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):
    '''
    Input: 
        source: a string corresponding to the string you are starting with
        target: a string corresponding to the string you want to end with
        ins_cost: an integer setting the insert cost
        del_cost: an integer setting the delete cost
        rep_cost: an integer setting the replace cost
    Output:
        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances
        med: the minimum edit distance (med) required to convert the source string to the target
    '''
    # use deletion and insert cost as  1
    m = len(source) 
    n = len(target) 
    #initialize cost matrix with zeros and dimensions (m+1,n+1) 
    D = np.zeros((m+1, n+1), dtype=int) 
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # Fill in column 0, from row 1 to row m, both inclusive
    for row in range(1,m+1): # Replace None with the proper range
        D[row,0] = D[row-1,0]+del_cost
        
    # Fill in row 0, for all columns from 1 to n, both inclusive
    for col in range(1,n+1): # Replace None with the proper range
        D[0,col] = D[0,col-1]+ins_cost
        
    # Loop through row 1 to row m, both inclusive
    for row in range(1,m+1):
        
        # Loop through column 1 to column n, both inclusive
        for col in range(1,n+1):
            
            # Intialize r_cost to the 'replace' cost that is passed into this function
            r_cost = D[row-1,col-1] + (rep_cost if source[row-1]!=target[col-1] else 0)
            
            # Update the cost at row, col based on previous entries in the cost matrix
            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)
            D[row,col] = min(r_cost, D[row-1,col]+del_cost, D[row,col-1]+ins_cost)
            
    # Set the minimum edit distance with the cost found at row m, column n 
    med = D[m,n]
    
    ### END CODE HERE ###
    return D, med
```

## Week2 Markov Chains and POS Tags

### Part of speech (POS) tagging
词性标注

对一段文字的每个词进行标注，这些词可能直接来自于字典，或是有字典中的词添加特殊词性的前后缀得来

可以用以上方式对每个词做出一个标注

### Markov chains
马尔可夫链（Markov Chain）是一种随机过程模型，描述了系统在离散时间内从一个状态转移到另一个状态的概率。其核心特性是马尔可夫性质，即系统的未来状态仅依赖于当前状态，与过去的状态无关。这种“无记忆性”使得马尔可夫链在概率论、机器学习和数据建模中具有广泛的应用。


马尔可夫链由以下要素组成：

- 状态空间：系统可能的所有状态集合。

- 转移概率矩阵：描述从一个状态转移到另一个状态的概率。例如，矩阵中的元素 (P(i, j)) 表示从状态 (i) 转移到状态 (j) 的概率。

- 初始状态分布：系统在初始时刻的状态概率分布。

马尔可夫链的一个重要性质是，当转移概率矩阵满足一定条件时，经过多次状态转移，系统的状态分布会收敛到一个稳定的分布，与初始状态无关。

举例，转移矩阵如下：

一行表示从左侧的状态转移到右侧各个状态的概率，一行之和总为1。$\pi$ 表示初始状态
![alt text](afflix/image-5.png)

state :
$Q=\{q_1,q_2,...,q_N\}$

$$ \text{Transition matrix} $$
$$ {A=\begin{Bmatrix}
 a_{1,1} & \dots  & a_{1,N}\\
 \vdots  & \ddots  & \vdots \\
 a_{N,1} & \dots & a_{N,N}
\end{Bmatrix}} $$

### 隐马尔可夫模型(HMM)

HMM 中定义了：
发射概率（Emission Probability）描述了在特定的隐藏状态下生成观测值的可能性。

在隐马尔可夫模型中，发射概率通常由一个发射矩阵表示，矩阵的行代表隐藏状态，列代表观测值，每个元素表示在特定隐藏状态下观测到特定观测值的概率

以下 going/to/eat 这些是可观测的，而动词/名词这些概念是隐状态

![alt text](afflix/image-6.png)

转移概率(是离散的(Discrete))，只与前一个状态有关\
$P(q_t|q_{t-1})=P(q_t|q_q,...,q_{t-1},y_1,...,y_{t-1})$

发射概率 (可以是连续的，可以是离散的), 只与前一个隐状态有关\
$P(y_t|q_t)=P(q_t|q_q,...,q_{t-1},q_t,y_1,...,y_{t-1})$

当发射概率是离散是，可以得到一个发射矩阵
$$ \text{Transition matrix} $$
$$ {B=\begin{Bmatrix}
 b_{1,1} & \dots  & b_{1,V}\\
 \vdots  & \ddots  & \vdots \\
 b_{N,1} & \dots & b_{N,V}
\end{Bmatrix}} $$

发射矩阵描述的是，从某个隐状态对应一个观测值的概率。

### 计算转移矩阵A(transition matrix)
根据语料库，计算转移矩阵的每一项，用如下公式：

- count occurrences of tag pairs  $C(t_{i-1},t_i)$
- calculate probability  $P(t_i|t_{i-1})=\frac{C(t_{i-1},t_i)+\epsilon}{ {\textstyle \sum_{j=1}^{N}} C(t_{i-1},t_j)+N*\epsilon}$

$\epsilon$ 是一种避免除以0的平滑技术

### 计算发射矩阵B(emission matrix)
方法与计算转移矩阵类似，把每项换成  $C(t_i,w_i)$\
$P(w_i|t_i)=\frac{C(t_i,w_i)+\epsilon}{ {\textstyle \sum_{j=1}^{V}} C(t_i,w_j)+N*\epsilon}=\frac{C(t_i,w_i)+\epsilon}{ C(t_i)+N*\epsilon}$


### Viterbi 算法
维特比算法是用DP来解决hmm的预测问题（或称为解码问题）的，即给定模型 $\lambda=(A,B,\pi)$和观察序列$O=(o_1,o_2,...,o_T)$,求对给定观察序列条件概率$P=(I|O)$最大的状态序列$I=(i_1,i_2,...,i_T)$

预测问题是hmm模型的3个基本问题之一，其他两个问题课程中没提到，可以参考<<统计学习方法>>。

为了求解这个问题，引入两个辅助矩阵，C和D，它们维度相同，都是N行（隐状态个数），T列（观测序列个数）。\
其中$c_{i,j}$ 表示给定模型$\lambda$，当前j项的观察序列为$O=(o_1,o_2,...,o_j)$，第$j$个观测项的对应的隐状态为$i$的最大概率是多少。\
$d_{i,j}$表示取到最大值为$c_{i,j}$的路径（从i-1）对应的状态值，即从在第j-1步时，从$d_{i,j}$状态到达第j步的i状态能够达到最大的概率$c_{i,j}$

$$ {C=\begin{Bmatrix}
c_{1,1} & \dots  &c_{1,T}\\
 \vdots  & \ddots  & \vdots \\
c_{N,1} & \dots &c_{N,T}
\end{Bmatrix}} 
{D=\begin{Bmatrix}
d_{1,1} & \dots  &d_{1,T}\\
 \vdots  & \ddots  & \vdots \\
d_{N,1} & \dots &d_{N,T}
\end{Bmatrix}}$$

Viterbi 算法分成3步
1. 初始化
   
   计算第一列: \
    $c_{1,i}=\pi_{i}*b_{i,cindex(w_1)}$， 其中$cindex(w_1)$表示w1对应的观测值的下标 \
    $d_{1,i}=0，表示前一项都是0（ $\pi$ 状态)

2. 向前递推

    对每个$1\le i\le N$ 和$j\gt 0$
   
   $c_{i,j}=\max\limits_{1\le k\le N}c_{k,j-1}\cdot a_{k,i}\cdot b_{i,\mathrm{cindex}(w_j)}$ \
   $d_{i,j}=\argmax\limits_{1\le k\le N}c_{k,j-1}\cdot a_{k,i}\cdot b_{i,cindex(w_j)}$

3. 向前递推
   
    根据D矩阵，从最后一列向前依次递推，获取最大每个i对应的最大概率的隐状态下标
    ![alt text](afflix/image-7.png)

实际计算过程中使用log函数，避免相乘之后的数值太小，丢失精度

$log(c_{i,j})=\max\limits_{1\le k\le N}log(c_{k,j-1})+ log(a_{k,i}) +log(b_{i,\mathrm{cindex}(w_j)})$
